{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistics Advance Part 1**"
      ],
      "metadata": {
        "id": "9_crtH5lWLwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a random variable in probability theory?"
      ],
      "metadata": {
        "id": "Wx7Jg4Y7WQVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "In probability theory, a random variable is a function that assigns a numerical value to each outcome in a sample space of a random experiment. This concept allows for the mathematical analysis of random phenomena by translating outcomes into numbers.\n",
        "\n",
        "A random variable is characterized by its probability distribution, which describes how probabilities are assigned to its possible values.\n",
        "Bookdown\n",
        "\n",
        "* Probability Mass Function (PMF):\n",
        "  \n",
        "  Used for discrete random variables, the PMF assigns probabilities to each possible value. For instance, in the case of a fair six-sided die, the PMF assigns a probability of 1/6 to each outcome.\n",
        "\n",
        "* Probability Density Function (PDF):\n",
        "\n",
        "  For continuous random variables, the PDF describes the likelihood of the variable taking on a particular value. The probability that the variable falls within a certain range is given by the area under the curve of the PDF over that range.\n",
        "\n"
      ],
      "metadata": {
        "id": "vV56Oz1qoucd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What are the types of random variables?"
      ],
      "metadata": {
        "id": "-XQ9k6vaWXNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "In probability theory, random variables are categorized based on the nature of their possible outcomes. The primary types are discrete, continuous random variables.\n",
        "\n",
        "* Discrete Random Variables\n",
        "\n",
        "  A discrete random variable can take on a finite or countably infinite number of distinct values. These values are often integers or whole numbers. For example, the number of heads in a series of coin flips or the number of defective items in a batch are discrete random variables.\n",
        "\n",
        "  * Key Characteristics:\n",
        "\n",
        "    * Countable outcomes\n",
        "\n",
        "    * Described by a Probability Mass Function (PMF)\n",
        "\n",
        "    * Examples: Binomial, Poisson, and Bernoulli distributions\n",
        "\n",
        "* Continuous Random Variables\n",
        "\n",
        "  A continuous random variable can take on an infinite number of values within a given range or interval. These values are often measurements and can be represented by real numbers. For instance, the height of individuals or the time it takes to complete a task are continuous random variables.\n",
        "\n",
        "  * Key Characteristics:\n",
        "\n",
        "    * Uncountably infinite outcomes\n",
        "\n",
        "    * Described by a Probability Density Function (PDF)\n",
        "\n",
        "    * Examples: Normal, Exponential, and Uniform distributions\n"
      ],
      "metadata": {
        "id": "MoVeKOwBplql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is the difference between discrete and continuous distributions?"
      ],
      "metadata": {
        "id": "pMTv7P3aWyEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "* Discrete Probability Distributions\n",
        "\n",
        "  * Nature of Outcomes: Discrete distributions deal with random variables that have countable, distinct outcomes. These outcomes are often integers or whole numbers.\n",
        "\n",
        "  * Probability Mass Function (PMF): The probability of each distinct outcome is assigned a specific probability. The sum of all probabilities equals 1.\n",
        "\n",
        "  * Examples:\n",
        "\n",
        "      * Binomial Distribution: Models the number of successes in a fixed number of independent Bernoulli trials.\n",
        "\n",
        "      * Poisson Distribution: Represents the number of events occurring within a fixed interval of time or space.\n",
        "\n",
        "      * Geometric Distribution: Describes the number of trials needed for the first success in a sequence of independent Bernoulli trials.\n",
        "\n",
        "  * Applications: Used in scenarios like counting the number of occurrences of an event, such as the number of customer arrivals at a store or the number of defective items in a batch.\n",
        "\n",
        "* Continuous Probability Distributions\n",
        "\n",
        "  * Nature of Outcomes: Continuous distributions are associated with random variables that can take on an infinite number of values within a given range. These values are typically real numbers.\n",
        "\n",
        "  * Probability Density Function (PDF): The probability of the random variable falling within a particular range is given by the area under the curve of the PDF over that range. The probability of the variable taking any exact value is zero.\n",
        "\n",
        "  * Examples:\n",
        "\n",
        "      * Normal Distribution: Describes data that clusters around a mean.\n",
        "\n",
        "      * Exponential Distribution: Models the time between events in a Poisson process.\n",
        "\n",
        "      * Uniform Distribution: All outcomes are equally likely within a certain range.\n",
        "\n",
        "  * Applications: Commonly used in fields like physics, engineering, and economics to model phenomena such as measurement errors, time intervals between events, and financial returns.\n",
        "\n"
      ],
      "metadata": {
        "id": "p4gUOWoVrDUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are probability distribution functions (PDF)?"
      ],
      "metadata": {
        "id": "Ut5AM1WBW0Mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "A Probability Density Function is a function that provides the relative likelihood for a continuous random variable to take on a given value. The value of the PDF at a specific point indicates how dense the probability is at that point. However, the probability of the random variable taking an exact value is always zero; instead, probabilities are determined over intervals. The total area under the curve of a PDF across the entire range of possible values is equal to 1, representing the certainty that the random variable takes some value within that range.\n",
        "\n"
      ],
      "metadata": {
        "id": "su3r8uEkr-Q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do cumulative distribution functions (CDF) differ from probability distribution functions (PDF)?"
      ],
      "metadata": {
        "id": "RsEwN4HsW5Oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "* Probability Density Function (PDF)\n",
        "\n",
        "  Definition: The PDF describes the relative likelihood for a continuous random variable to take on a particular value. For a given value x, the PDF f(x) represents the probability density at that point. The probability that the random variable falls within a specific interval is given by the area under the curve of the PDF over that interval.\n",
        "\n",
        "* Cumulative Distribution Function (CDF)\n",
        "\n",
        "  Definition: The CDF of a random variable X is a function that gives the probability that X will take a value less than or equal to x. It provides the cumulative probability up to a certain point.\n",
        "\n",
        "| Feature                        | Probability Density Function (PDF)                                         | Cumulative Distribution Function (CDF)                                                     |                                         |\n",
        "| ------------------------------ | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | --------------------------------------- |\n",
        "| **Definition**                 | Describes the likelihood of a random variable taking a specific value      | Describes the probability that a random variable is less than or equal to a specific value |                                         |\n",
        "| **Function Type**              | Function of a continuous random variable                                   | Function of a random variable (continuous or discrete)                                     |                                         |\n",
        "| **Probability Interpretation** | Probability density at a specific point (not probability)                  | Probability that the random variable is less than or equal to a specific value             |                                         |\n",
        "| **Range**                      | $[0, \\infty)$                                                              | $[0, 1]$                                                                                   |                                         |\n",
        "| **Behavior**                   | Can vary; not necessarily monotonic                                        | Non-decreasing                                                                             |                                         |\n",
        "| **Relationship**               | CDF is the integral of the PDF: $F_X(x) = \\int_{-\\infty}^{x} f_X(t) \\, dt$ | PDF is the derivative of the CDF: $f_X(x) = \\frac{d}{dx} F_X(x)$                           | ([ResearchGate][1], [Quality Gurus][2]) |\n",
        "\n",
        "[1]: https://www.researchgate.net/figure/Cumulative-distribution-and-probability-density-or-mass-functions-of-random-variables-a_fig1_314276613?utm_source=chatgpt.com \"probability density or mass functions ...\"\n",
        "[2]: https://www.qualitygurus.com/pdf-cdf-and-pmf-probability-distribution-functions/?utm_source=chatgpt.com \"PDF, CDF and PMF – Probability Distribution Functions | Quality Gurus\"\n"
      ],
      "metadata": {
        "id": "pPPE6Uz0sHs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a discrete uniform distribution?"
      ],
      "metadata": {
        "id": "_3h5LZIEXAR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "A discrete uniform distribution is a type of probability distribution where a finite number of equally likely outcomes are possible. In simpler terms, every outcome in the sample space has the same probability of occurring.\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "* Finite number of outcomes: The outcomes must be countable and limited.\n",
        "\n",
        "* Equal probability: Each outcome has the same chance (probability) of occurring."
      ],
      "metadata": {
        "id": "UYwj4LKf4Rj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What are the key properties of a Bernoulli distribution?"
      ],
      "metadata": {
        "id": "wKrjvgvZXElW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "The Bernoulli distribution is one of the simplest and most fundamental probability distributions in statistics and probability theory. It describes a random experiment that has exactly two possible outcomes: success (usually coded as 1) and failure (coded as 0).\n",
        "\n",
        "Key Properties of the Bernoulli Distribution:\n",
        "1. Two Outcomes\n",
        "\n",
        "  The random variable X can take on only two values:\n",
        "                    𝑋∈{0,1}\n",
        "  * X=1: Success (e.g., heads, win, yes)\n",
        "  * X=0: Failure (e.g., tails, loss, no)\n",
        "\n",
        "2. Probability Mass Function (PMF)\n",
        "\n",
        "      𝑃(𝑋=𝑥) = { 𝑝     if 𝑥=1\n",
        "\n",
        "                1−𝑝   if 𝑥=0 }\n",
        "\n",
        "    or written compactly:\n",
        "\n",
        "      P(X=x)=p^x (1−p)^1−x, for x∈{0,1}\n",
        "\n",
        "  * p is the probability of success (i.e., P(X=1))\n",
        "  * 0≤p≤1\n",
        "\n",
        "3. Mean (Expected Value)\n",
        "\n",
        "        E[X]=p\n",
        "\n",
        "4. Variance\n",
        "\n",
        "        Var(X) = p(1-p)\n",
        "\n",
        "5. Support\n",
        "\n",
        "    The distribution is defined on the discrete set {0,1}\n",
        "\n",
        "6. Skewness\n",
        "  * The distribution is symmetric when p=0.5\n",
        "  * Skewed right when p<0.5\n",
        "  * Skewed left when p>0.5\n"
      ],
      "metadata": {
        "id": "8YEtjky-4qsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the binomial distribution, and how is it used in probability?"
      ],
      "metadata": {
        "id": "LAH6lon4XKfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "The binomial distribution is a discrete probability distribution that models the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success.\n",
        "\n",
        "The binomial distribution is used when:\n",
        "1. There are a fixed number of independent trials.\n",
        "2. Each trial has only two outcomes (success/failure).\n",
        "3. The probability of success stays constant across trials."
      ],
      "metadata": {
        "id": "ISZ0f9xP-Wls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the Poisson distribution and where is it applied?"
      ],
      "metadata": {
        "id": "mcfrRoEsXRqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "The Poisson distribution is a discrete probability distribution used to model the number of events that occur in a fixed interval of time or space, when those events happen independently and at a constant average rate.\n",
        "\n",
        "Probability Mass Function (PMF)"
      ],
      "metadata": {
        "id": "x0d0Zz_z-1Y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is a continuous uniform distribution?"
      ],
      "metadata": {
        "id": "dCTB-wN4XWVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "The continuous uniform distribution is one of the simplest continuous probability distributions. It models a situation where every value within a given interval is equally likely to occur."
      ],
      "metadata": {
        "id": "KgNA0M5W_AyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are the characteristics of a normal distribution?"
      ],
      "metadata": {
        "id": "LaMWruxOXcof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "The normal distribution, also known as the Gaussian distribution, is one of the most important and widely used probability distributions in statistics. It describes how values are distributed when many small, independent effects add up.\n",
        "\n",
        "1. Bell-Shaped Curve\n",
        "  * The graph of the normal distribution is symmetric, unimodal, and bell-shaped.\n",
        "  * It is centered at the mean μ.\n",
        "\n",
        "2. Defined by Two Parameters\n",
        "  * μ (mu): Mean — determines the center of the distribution.\n",
        "  * σ (sigma): Standard deviation — controls the spread or width of the curve.\n",
        "\n",
        "          X∼N(μ,σ^2)\n"
      ],
      "metadata": {
        "id": "a4tPCvwj_Tas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the standard normal distribution, and why is it important?"
      ],
      "metadata": {
        "id": "YKCQZpecXqpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "The standard normal distribution is a special case of the normal distribution that has:\n",
        "  * Mean μ=0\n",
        "  * Standard deviation σ=1\n",
        "\n",
        "It is denoted by:\n",
        "\n",
        "        𝑍∼𝑁(0,1)\n"
      ],
      "metadata": {
        "id": "s7OKUSE-AHwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the Central Limit Theorem (CLT), and why is it critical in statistics?"
      ],
      "metadata": {
        "id": "NZ3AIURIYEzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "The Central Limit Theorem (CLT) is one of the most important theorems in statistics. It explains why the normal distribution appears so frequently in statistical practice — even when the data itself is not normally distributed.\n",
        "\n",
        "The CLT states that:\n",
        "\n",
        "When independent random variables are added, their normalized sum tends toward a normal distribution, regardless of the original distribution, provided the sample size is sufficiently large.\n",
        "\n",
        "Formally, if X1,X2,…,Xn are i.i.d. random variables with:"
      ],
      "metadata": {
        "id": "3WtnWTCdA85G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How does the Central Limit Theorem relate to the normal distribution?"
      ],
      "metadata": {
        "id": "w_JZmO3tYKYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "The Central Limit Theorem (CLT) and the normal distribution are deeply connected — the CLT explains why the normal distribution is so prevalent in statistics.\n",
        "\n",
        "1. CLT Explains the Emergence of the Normal Distribution\n",
        "The CLT states that as the sample size increases, the sampling distribution of the sample mean (or sum) of any population (with finite mean and variance) approaches a normal distribution, regardless of the shape of the original population distribution.\n",
        "2. Enables Use of Normal-Based Methods\n",
        "\n",
        "  Because of the CLT, we can:\n",
        "\n",
        "* Use normal probability models to estimate population parameters (e.g., confidence intervals)\n",
        "* Conduct z-tests and t-tests\n",
        "* Apply control charts in quality control\n",
        "* Build regression models based on normally-distributed errors\n",
        "\n",
        "Even if the population is not normal, the sampling distribution of the mean will behave as if it is — provided the sample is large enough.\n",
        "3. Sampling Distribution Is Normal — Not the Original Data\n",
        "\n",
        "  It's crucial to understand:\n",
        "* The original population data might not be normally distributed.\n",
        "* But the distribution of the sample mean (or sum) will be approximately normal — that’s the power of the CLT."
      ],
      "metadata": {
        "id": "EJWSc94RBhZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is the application of Z statistics in hypothesis testing?"
      ],
      "metadata": {
        "id": "Vl4V00EFYOgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "Z-statistics (or Z-scores) are widely used in hypothesis testing when we are dealing with normally distributed data or can invoke the Central Limit Theorem. They allow us to determine how many standard deviations a sample statistic (like a sample mean or proportion) is from the population parameter under the null hypothesis."
      ],
      "metadata": {
        "id": "QD3MLCK-CLW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How do you calculate a Z-score, and what does it represent?"
      ],
      "metadata": {
        "id": "cmKGJ0i6YS-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "A Z-score (also called a standard score) tells you how many standard deviations a data point is from the mean of a distribution.\n",
        "\n",
        "Z-Score Represents\n",
        "* Z = 0 → The value is exactly at the mean\n",
        "* Z > 0 → The value is above the mean\n",
        "* Z < 0 → The value is below the mean\n",
        "* |Z| > 2 → The value is unusual or rare in a normal distribution"
      ],
      "metadata": {
        "id": "H2HoVZvQCY3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are point estimates and interval estimates in statistics?"
      ],
      "metadata": {
        "id": "sc-7RYtZYaFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "In statistics, point estimates and interval estimates are two different ways of estimating an unknown population parameter (like the population mean, proportion, or standard deviation) based on sample data.\n",
        "\n",
        "1. Point Estimate\n",
        "\n",
        "    A point estimate is a single value used as an estimate of a population parameter.\n",
        "\n",
        " Examples:\n",
        "  * Sample mean Xˉas a point estimate for the population mean μ\n",
        "  * Sample proportion p^ as a point estimate for the population proportion p\n",
        "\n",
        "2. Interval Estimate (Confidence Interval)\n",
        "\n",
        "  An interval estimate provides a range of values within which the true population parameter is likely to fall, along with a confidence level (e.g., 95%).\n",
        "\n",
        "General form:\n",
        "\n",
        "      Interval Estimate = Point Estimate ± Margin of Error"
      ],
      "metadata": {
        "id": "OsjvegA6C4vP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the significance of confidence intervals in statistical analysis?"
      ],
      "metadata": {
        "id": "CMN4pkiKYefN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "Confidence intervals (CIs) are critical tools in statistical analysis because they provide more information than simple point estimates. Instead of giving just a single number, they give a range of plausible values for an unknown population parameter, along with a level of certainty.\n",
        "\n",
        "1. Quantify Uncertainty\n",
        "  * A point estimate (like a sample mean) tells you the best guess.\n",
        "  * A confidence interval shows the range of values that are likely to contain the true parameter.\n",
        "  * This reflects the inherent variability in using samples to estimate population values.\n",
        "\n",
        "2. Support Better Decision-Making\n",
        "  * CIs help determine how precise an estimate is.\n",
        "  * A narrow CI indicates more precision; a wide CI indicates more uncertainty.\n",
        "  * Decision-makers can assess the risk and reliability of conclusions.\n",
        "\n",
        "3. Enable Hypothesis Testing Without a P-Value\n",
        "\n",
        "  If a confidence interval does not include the null value (e.g., 0 for mean difference or 1 for odds ratio), it suggests the result is statistically significant.\n",
        "\n",
        "4. Adaptable Across Statistical Contexts\n",
        "\n",
        "  Confidence intervals are used in estimating:\n",
        "  * Means\n",
        "  * Proportions\n",
        "  * Regression coefficients\n",
        "  * Differences between groups\n",
        "  * Odds ratios and risk ratios in epidemiology\n",
        "  "
      ],
      "metadata": {
        "id": "Z9FQnwl-EOeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What is the relationship between a Z-score and a confidence interval?"
      ],
      "metadata": {
        "id": "16bW9K5TYkzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "Great question! The Z-score and confidence interval (CI) are closely related because Z-scores are used to construct confidence intervals when the population standard deviation is known or the sample size is large.\n",
        "\n",
        "1. Z-score represents the critical value from the standard normal distribution corresponding to the desired confidence level.\n",
        "2. Confidence intervals are constructed using the point estimate ± (Z-score × standard error):\n",
        "\n",
        "      CI = point Estimate ± Z* ×Standard Error\n"
      ],
      "metadata": {
        "id": "wohQExZfFHo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How are Z-scores used to compare different distributions?"
      ],
      "metadata": {
        "id": "98Z9irGrYpyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "Z-scores are extremely useful for comparing values from different distributions, even if those distributions have different means or standard deviations. They allow you to place data points on a common scale, which makes comparisons fair and meaningful."
      ],
      "metadata": {
        "id": "783atuHOF0M4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What are the assumptions for applying the Central Limit Theorem?"
      ],
      "metadata": {
        "id": "2UFMQwGOYwIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "The Central Limit Theorem (CLT) is powerful, but it relies on a few key assumptions to hold true. These assumptions ensure that the sampling distribution of the sample mean (or sum) will approximate a normal distribution, even if the population distribution is not normal.\n",
        "\n",
        "1. Independence of Observations\n",
        "\n",
        "  * Each observation in the sample must be independent of the others.\n",
        "  * In practice, this usually means:\n",
        "      * Random sampling or random assignment\n",
        "      * No repeated measures or related subjects (unless adjustments are made)\n",
        "\n",
        "2. Identically Distributed\n",
        "* All sampled observations should come from the same population (i.e., they share the same mean and standard deviation).\n",
        "\n",
        "3. Finite Mean and Variance\n",
        "* The population from which samples are drawn must have a finite mean μ and a finite variance σ^2.\n",
        "\n",
        "4. Sample Size (n) is \"Large Enough\"\n",
        "* For most distributions, a sample size of n≥30 is typically considered sufficient.\n",
        "* Heavily skewed or non-normal distributions may require larger sample sizes."
      ],
      "metadata": {
        "id": "TdPevJIIGAlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. What is the concept of expected value in a probability distribution?"
      ],
      "metadata": {
        "id": "bBLB_xTWY2fU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "The expected value (often denoted as E[X] or μ) is a fundamental concept in probability and statistics. It represents the long-run average or mean outcome of a random variable if the experiment were repeated many times.\n",
        "\n",
        "The expected value gives a single summary number that describes the center or \"average\" of a probability distribution."
      ],
      "metadata": {
        "id": "JfZ_UQK7G5uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. How does a probability distribution relate to the expected outcome of a random variable?"
      ],
      "metadata": {
        "id": "VUIBGxS-Y9IM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->>\n",
        "\n",
        "A probability distribution defines how likely different outcomes are for a random variable. The expected outcome (or expected value) is a summary measure that captures the center or long-run average of that distribution.\n",
        "\n",
        "Relationship Between Probability Distribution and Expected Value\n",
        "\n",
        "The expected value of a random variable is calculated using its probability distribution. Specifically:\n",
        "\n",
        "* For discrete random variables, each possible value is weighted by its probability.\n",
        "\n",
        "* For continuous random variables, the values are weighted by their probability density over an interval.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lu-NSIXWHM8I"
      }
    }
  ]
}